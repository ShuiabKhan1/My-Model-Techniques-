{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ac8b125",
   "metadata": {},
   "source": [
    "Feature Importance that i have  and can be used post transformation steps (scalar, One hot, Normalization, resampling,Missing values...etc) and before training model ...\n",
    "\n",
    "1. sklearns SelectKbest(chi2 ,mutual info ..etc ) c2 for linear correlation var vs target (pvalue, 2 variables are indepen)\n",
    "  mutual info - information theory scoring - Non Linear correl )\n",
      "we need to be careful here because this just gives us top features but we might not need features with same score instead use non redundant ones, then we go for 5th option \n", 
    "2. sklearns permutation_importance\n",
    "\n",
    "3. Models sklearns feature_importances_ with diff models  randamForest - impurities, LogReg, DT...etc \n",
    "\n",
    "4. Recursive Feature Elimination  - decription is written above \n",
    "\n",
    "5. remove redundant features (same importance)\n",
    "\n",
    "5. Dimentional Reduction (  PCA )- Unsupervised, new features created - Not recommended if interested in same features- More columns ",
 
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb7169c",
   "metadata": {},
   "source": [
    "There are different ways to get feature importance for a model with too many columns. Here are a few common approaches:\n",
    "\n",
    "1. Random Forest: One of the advantages of using Random Forest algorithm is that it provides a built-in feature importance metric based on the reduction in impurity achieved by each feature. You can access the feature importance scores using the `feature_importances_` attribute of the Random Forest model. Here's an example:\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "The `importances` variable will contain an array of feature importances for each column in `X_train`.\n",
    "\n",
    "2. Permutation Importance: Another approach is to use Permutation Importance to measure the importance of features. Permutation Importance works by permuting the values of each feature column in the dataset, and measuring the impact on the model's performance. Scikit-learn provides an implementation of this approach through the `PermutationImportance` class. Here's an example:\n",
    "\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(rf, X_train, y_train, n_repeats=10, random_state=0)\n",
    "\n",
    "importances = result.importances_mean\n",
    "\n",
    "The `importances` variable will contain an array of feature importances for each column in `X_train`.\n",
    "\n",
    "3. Recursive Feature Elimination (RFE): RFE is an iterative approach that works by removing the least important features recursively until a desired number of features is reached. Scikit-learn provides a class called `RFECV` (Recursive Feature Elimination with Cross Validation) that performs RFE while using cross-validation to evaluate the model. Here's an example:\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "rfecv = RFECV(estimator=rf, step=1, cv=10, scoring='r2')\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "importances = rfecv.support_\n",
    "\n",
    "The `importances` variable will contain a boolean mask indicating which columns are considered important by the `RFECV` algorithm.\n",
    "\n",
    "These are just a few examples of how you can obtain feature importance for a high-dimensional dataset. Depending on your specific use case, you may need to experiment with different algorithms and approaches to understand the underlying structure of your data and identify the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad1c60a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
